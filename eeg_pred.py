# -*- coding: utf-8 -*-
"""EEG_pred.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1IngdYplVujBXM_aLKI3JahV3z0Q0ywm1
"""

import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

df = pd.read_csv('/content/Epileptic Seizure Recognition.csv')

df

x= df.drop(["y"], axis=1)
y= df["y"]

signal = x.sample(1).values.flatten()

len(signal)



signal_numeric = pd.to_numeric(signal, errors='coerce')
signal_numeric = signal_numeric[~np.isnan(signal_numeric)]

plt.figure(figsize=(12, 6))
plt.plot(
    np.arange(len(signal_numeric)),
    signal_numeric,
    marker="o",
    linestyle="-",
    markersize=4,
    label="Sampled Signal",
)

plt.xlabel("Time")
plt.ylabel("Amplitude")
plt.title("Sampled Signal")
plt.grid(True)
plt.legend()
plt.show()

def plot(data, df, color, ax=None):
    if ax is None:
        fig, ax = plt.subplots()

    fig = ax.figure
    fig.set_facecolor((0, 0, 1, 0.4))

    ax.plot(df[data], linestyle='-', color=color, alpha=0.4)
    ax.set_xlabel("Time")
    ax.set_ylabel(data)
    ax.grid(True)

    if ax is None:
        plt.show()

w = 100
t = np.linspace(0, 11500, 11500)
sin = 1000*np.sin(2 * np.pi * w * t)

len(t)

# plt.plot(sin)

len(sin)

fig , axes= plt.subplots(3, 2, figsize=(30, 10))
axes[0][0].plot(sin, linestyle='-', color='blue', alpha=0.8)
plot("X1", x, "red", axes[0][0])
plot("X2", x, "blue", axes[0][1])
plot("X150", x, "red", axes[1][0])
plot("X149", x, "blue", axes[1][1])
plot("X148", x, "red", axes[2][0])
plot("X147", x, "blue", axes[2][1])
plt.show()

def convert_fourier(signal):
  """Converts a signal to its Fourier transform.

  Args:
    signal: The input signal (can be a single value or an array-like).

  Returns:
    The Fourier transform of the signal.
  """
  signal_numeric = pd.to_numeric(signal, errors='coerce')

  if np.isscalar(signal_numeric):
    if np.isnan(signal_numeric):
      return np.nan
    else:

      fft_values = np.fft.fft([signal_numeric])

  else:
    signal_numeric = signal_numeric[~np.isnan(signal_numeric)]
    fft_values = np.fft.fft(signal_numeric)


  return np.array(abs(fft_values))

df_copy = df.copy()

column_names = df.columns
columns = column_names[1:179]

columns

df_copy = df.copy()

column_names = df.columns
columns = column_names[1:179]


for i in columns:
  df_copy[i] = df_copy[i].apply(convert_fourier)
  df_copy[i] = df_copy[i].apply(lambda x: x[0] if isinstance(x, np.ndarray) else x)

# df_copy['X1'] = df_copy['X1'].apply(lambda x: x[0])

# print(df_copy['X1'])

df_copy

sns.boxplot(x=len, y="X1", data=df_copy)
plt.show()

x= df_copy.drop(["y", "Unnamed"], axis=1)
y= df_copy["y"]

from sklearn.preprocessing import StandardScaler, Normalizer
sc = StandardScaler()
nr= Normalizer()

x_standard = pd.DataFrame(sc.fit_transform(x))

df_new = pd.concat([x_standard, y], axis=1)

df_new

sns.boxplot(x=len, y=x_standard[0], data=df_copy)
plt.show()

sns.kdeplot(x_standard[0])
plt.show()

def remove_outliers(df, column):
    Q1 = df[column].quantile(0.25)
    Q3 = df[column].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    df = df[(df[column] >= lower_bound) & (df[column] <= upper_bound)]
    return df

# for i in range(177):
#   df_new = remove_outliers(df_new, i)

df_new.shape

df_copy.shape

sns.kdeplot(df_new[0])
plt.show()

x_normalized = pd.DataFrame(nr.fit_transform(x))

from sklearn.preprocessing import OneHotEncoder

y_onehot = pd.DataFrame(OneHotEncoder().fit_transform(y.values.reshape(-1, 1)).toarray())

y_onehot

sns.kdeplot((x_normalized[0]))
plt.show()

from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(x_standard, y_onehot, test_size=0.2, random_state=42)

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score

# rf = RandomForestClassifier(n_estimators=200, random_state=42)
# rf.fit(x_train, y_train)

# y_pred = rf.predict(x_test)

# accuracy_score(y_test, y_pred)

from sklearn.model_selection import GridSearchCV
from sklearn.metrics import mean_squared_error
from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier

param_grid = {
    'max_depth': [8,10,15, 20, 30, None],
    'min_samples_split': [2, 5, 10,15,12],
}
dtree_cla = DecisionTreeClassifier(random_state=42)
grid_search = GridSearchCV(estimator=dtree_cla, param_grid=param_grid,
                           cv=5, n_jobs=-1, verbose=2, scoring='neg_mean_squared_error')
grid_search.fit(x_train, y_train)
best_dtree_cls = grid_search.best_estimator_
y_pred = best_dtree_cls.predict(x_test)
params = grid_search.best_params_
print(f"Best Parameters: {params}")
print(best_dtree_cls)

dtree= DecisionTreeClassifier(max_depth=8, min_samples_split=15)
dtree.fit(x_train, y_train)

y_pred_dtree= dtree.predict(x_test)

# for i in range(len(y_pred)):
#   max = np.max(y_pred[i])
#   for j in range(len(y_pred[i])):
#     if y_pred[i][j] == max:
#       y_pred[i][j] =1
#     else:
#       y_pred[i][j] =0

y_pred_dtree

accuracy_score(y_test, y_pred_dtree)

param_grid = {
    'n_estimators': [50, 100, 150, 200, 250, None]
}
r_reg = RandomForestClassifier(random_state=42)
grid_search = GridSearchCV(estimator=r_reg, param_grid=param_grid,
                           cv=5, n_jobs=-1, verbose=2, scoring='neg_mean_squared_error')
grid_search.fit(x_train, y_train)
best_r_classifier= grid_search.best_estimator_
y_pred = best_r_classifier.predict(x_test)
params = grid_search.best_params_
print(f"Best Parameters: {params}")
print(best_r_classifier)

r_cl = RandomForestClassifier(n_estimators=50, random_state=42)
r_cl.fit(x_train, y_train)

y_pred_ran= r_cl.predict(x_test)

accuracy_score(y_pred_ran, y_test)

import tensorflow as tf
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import  LSTM, Dense, GRU, Bidirectional, Input
from sklearn.model_selection import train_test_split
from tensorflow.keras.models import Model
from tensorflow.keras.utils import to_categorical

y_train.shape

input = Input(shape=(178,))
dense1 = Dense(100, activation='relu')(input)
reshape = tf.keras.layers.Reshape((1, 100))(dense1)
lstm0= Bidirectional(LSTM(100, return_sequences=True))(reshape)
lstm1= LSTM(100, return_sequences=True)(lstm0)
lstm2= LSTM(100, return_sequences=False)(lstm1)
output = Dense(5, activation='softmax')(lstm2)
model = Model(inputs=input, outputs=output)

model.summary()

from tensorflow.keras.callbacks import EarlyStopping
es = EarlyStopping(monitor='val_loss', mode='min', verbose=1)

model.compile(optimizer ='adam', loss='categorical_crossentropy', metrics=['accuracy'])

history= model.fit(x_train, y_train, epochs=100, validation_data=(x_test, y_test), callbacks=[es])

plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'valid'], loc='upper right')







